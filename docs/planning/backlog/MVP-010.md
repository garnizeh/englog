# MVP-010: OpenAI API Integration

**Feature:** MVP-AI
**Priority:** P0 (Critical)
**Effort:** Medium (3 days)
**Dependencies:** MVP-009 (gRPC Worker Pool)

## Objective

Integrate OpenAI GPT-4 API for advanced sentiment analysis and content processing, upgrading from the basic Ollama implementation used in Phase 0 prototype.

## Technical Scope

### API Integration

- OpenAI client configuration and authentication
- GPT-4 model integration for content analysis
- Rate limiting and quota management
- Error handling and retry logic
- Cost monitoring and usage tracking

### AI Processing Pipeline

- Prompt engineering for journal analysis
- Response parsing and validation
- Result caching to reduce API costs
- Fallback mechanisms for service unavailability
- Multi-model support (GPT-4, GPT-3.5-turbo)

### Service Architecture

- AI service abstraction layer
- Provider selection logic
- Configuration management
- Circuit breaker implementation
- Performance monitoring

## Acceptance Criteria

### Functional Requirements

- [ ] OpenAI GPT-4 successfully processes journal entries
- [ ] Sentiment analysis returns accurate scores (-1 to +1)
- [ ] Content extraction identifies key themes and concepts
- [ ] Response parsing handles various OpenAI response formats
- [ ] Error handling gracefully manages API failures

### Performance Requirements

- [ ] API responses received within 10 seconds for standard entries
- [ ] Rate limiting prevents API quota exhaustion
- [ ] Response caching reduces redundant API calls
- [ ] Circuit breaker protects against cascading failures
- [ ] Cost tracking monitors usage and spending

### Reliability Requirements

- [ ] Retry logic handles temporary API failures
- [ ] Fallback to Ollama when OpenAI unavailable
- [ ] Graceful degradation during high load
- [ ] API key validation and rotation support
- [ ] Comprehensive error logging and monitoring

## Implementation Details

### OpenAI Client Service

````go
type OpenAIService struct {
    client        *openai.Client
    models        ModelConfig
    rateLimit     *rate.Limiter
    circuitBreaker *CircuitBreaker
    cache         Cache
    logger        *slog.Logger
    metrics       *AIMetrics
}

type ModelConfig struct {
    PrimaryModel   string  // "gpt-4-turbo"
    FallbackModel  string  // "gpt-3.5-turbo"
    MaxTokens      int     // 1000
    Temperature    float32 // 0.3
    TopP           float32 // 0.8
    FrequencyPenalty float32 // 0.0
    PresencePenalty  float32 // 0.0
}

type ProcessingRequest struct {
    Content   string            `json:"content"`
    Type      ProcessingType    `json:"type"`
    UserID    string            `json:"user_id"`
    JournalID string            `json:"journal_id"`
    Metadata  map[string]string `json:"metadata"`
}

type ProcessingResult struct {
    Sentiment     SentimentResult   `json:"sentiment"`
    Emotions      []EmotionResult   `json:"emotions"`
    Themes        []ThemeResult     `json:"themes"`
    KeyPhrases    []string          `json:"key_phrases"`
    Entities      []EntityResult    `json:"entities"`
    Summary       string            `json:"summary"`
    Confidence    float64           `json:"confidence"`
    ProcessingTime time.Duration    `json:"processing_time"`
    Model         string            `json:"model"`
    TokensUsed    int               `json:"tokens_used"`
    Cost          float64           `json:"cost_usd"`
}

func NewOpenAIService(apiKey string, config ModelConfig) *OpenAIService {
    client := openai.NewClient(apiKey)

    return &OpenAIService{
        client:         client,
        models:         config,
        rateLimit:      rate.NewLimiter(rate.Every(time.Minute), 50), // 50 requests per minute
        circuitBreaker: NewCircuitBreaker(CircuitBreakerConfig{
            FailureThreshold: 5,
            RecoveryTimeout:  30 * time.Second,
        }),
        cache:          NewRedisCache(),
        logger:         slog.Default(),
    }
}

func (s *OpenAIService) ProcessContent(ctx context.Context, req *ProcessingRequest) (*ProcessingResult, error) {
    start := time.Now()

    // Check rate limiting
    if !s.rateLimit.Allow() {
        return nil, ErrRateLimitExceeded
    }

    // Check cache first
    cacheKey := s.generateCacheKey(req)
    if cached, found := s.cache.Get(ctx, cacheKey); found {
        s.metrics.CacheHits.Inc()
        return cached.(*ProcessingResult), nil
    }

    // Process with circuit breaker
    result, err := s.circuitBreaker.Execute(func() (any, error) {
        return s.processWithOpenAI(ctx, req)
    })

    if err != nil {
        s.logger.Error("OpenAI processing failed", "error", err, "journal_id", req.JournalID)
        return nil, err
    }

    processingResult := result.(*ProcessingResult)
    processingResult.ProcessingTime = time.Since(start)

    // Cache the result
    s.cache.Set(ctx, cacheKey, processingResult, 1*time.Hour)

    return processingResult, nil
}

func (s *OpenAIService) processWithOpenAI(ctx context.Context, req *ProcessingRequest) (*ProcessingResult, error) {
    prompt := s.buildPrompt(req)

    chatReq := openai.ChatCompletionRequest{
        Model: s.models.PrimaryModel,
        Messages: []openai.ChatCompletionMessage{
            {
                Role:    openai.ChatMessageRoleSystem,
                Content: s.getSystemPrompt(),
            },
            {
                Role:    openai.ChatMessageRoleUser,
                Content: prompt,
            },
        },
        MaxTokens:        s.models.MaxTokens,
        Temperature:      s.models.Temperature,
        TopP:             s.models.TopP,
        FrequencyPenalty: s.models.FrequencyPenalty,
        PresencePenalty:  s.models.PresencePenalty,
    }

    resp, err := s.client.CreateChatCompletion(ctx, chatReq)
    if err != nil {
        // Try fallback model if primary fails
        if s.shouldUseFallback(err) {
            chatReq.Model = s.models.FallbackModel
            resp, err = s.client.CreateChatCompletion(ctx, chatReq)
        }

        if err != nil {
            return nil, fmt.Errorf("OpenAI API error: %w", err)
        }
    }

    if len(resp.Choices) == 0 {
        return nil, fmt.Errorf("no response from OpenAI")
    }

    result, err := s.parseResponse(resp.Choices[0].Message.Content)
    if err != nil {
        return nil, fmt.Errorf("failed to parse response: %w", err)
    }

    // Add metadata
    result.Model = chatReq.Model
    result.TokensUsed = resp.Usage.TotalTokens
    result.Cost = s.calculateCost(chatReq.Model, resp.Usage.TotalTokens)

    s.metrics.TokensUsed.Add(float64(resp.Usage.TotalTokens))
    s.metrics.APICalls.Inc()
    s.metrics.CostTotal.Add(result.Cost)

    return result, nil
}

func (s *OpenAIService) getSystemPrompt() string {
    return `You are an expert at analyzing personal journal entries. Your task is to provide comprehensive analysis including sentiment, emotions, themes, and insights.

Please analyze the provided journal entry and return a JSON response with the following structure:

{
  "sentiment": {
    "score": 0.7,
    "label": "positive",
    "confidence": 0.9
  },
  "emotions": [
    {"name": "joy", "intensity": 0.8},
    {"name": "gratitude", "intensity": 0.6}
  ],
  "themes": [
    {"name": "personal_growth", "confidence": 0.9},
    {"name": "relationships", "confidence": 0.7}
  ],
  "key_phrases": ["feeling grateful", "positive change", "good day"],
  "entities": [
    {"text": "work", "type": "activity", "confidence": 0.8},
    {"text": "family", "type": "people", "confidence": 0.9}
  ],
  "summary": "A brief 1-2 sentence summary of the entry",
  "confidence": 0.85
}

Guidelines:
- Sentiment score should be between -1 (very negative) and +1 (very positive)
- Emotion intensities should be between 0 and 1
- Include confidence scores for uncertain classifications
- Be sensitive to personal and emotional content
- Focus on the writer's perspective and feelings`
}

func (s *OpenAIService) buildPrompt(req *ProcessingRequest) string {
    var prompt strings.Builder

    prompt.WriteString("Please analyze this journal entry:\n\n")
    prompt.WriteString(req.Content)

    // Add context if available
    if req.Metadata != nil {
        if mood, exists := req.Metadata["mood"]; exists {
            prompt.WriteString(fmt.Sprintf("\n\nUser's self-reported mood: %s", mood))
        }
        if tags, exists := req.Metadata["tags"]; exists {
            prompt.WriteString(fmt.Sprintf("\nUser-added tags: %s", tags))
        }
    }

    return prompt.String()
}

func (s *OpenAIService) parseResponse(content string) (*ProcessingResult, error) {
    // Clean up response (remove markdown formatting if present)
    content = strings.TrimPrefix(content, "```json")
    content = strings.TrimSuffix(content, "```")
    content = strings.TrimSpace(content)

    var response struct {
        Sentiment  SentimentResult   `json:"sentiment"`
        Emotions   []EmotionResult   `json:"emotions"`
        Themes     []ThemeResult     `json:"themes"`
        KeyPhrases []string          `json:"key_phrases"`
        Entities   []EntityResult    `json:"entities"`
        Summary    string            `json:"summary"`
        Confidence float64           `json:"confidence"`
    }

    if err := json.Unmarshal([]byte(content), &response); err != nil {
        return nil, fmt.Errorf("invalid JSON response: %w", err)
    }

    // Validate response
    if err := s.validateResponse(&response); err != nil {
        return nil, fmt.Errorf("invalid response format: %w", err)
    }

    return &ProcessingResult{
        Sentiment:  response.Sentiment,
        Emotions:   response.Emotions,
        Themes:     response.Themes,
        KeyPhrases: response.KeyPhrases,
        Entities:   response.Entities,
        Summary:    response.Summary,
        Confidence: response.Confidence,
    }, nil
}

func (s *OpenAIService) calculateCost(model string, tokens int) float64 {
    // Pricing as of 2024 (per 1000 tokens)
    pricing := map[string]struct{ input, output float64 }{
        "gpt-4-turbo":     {input: 0.01, output: 0.03},
        "gpt-3.5-turbo":   {input: 0.0005, output: 0.0015},
    }

    if rates, exists := pricing[model]; exists {
        // Estimate roughly 75% input, 25% output tokens
        inputTokens := float64(tokens) * 0.75
        outputTokens := float64(tokens) * 0.25

        return (inputTokens/1000)*rates.input + (outputTokens/1000)*rates.output
    }

    return 0 // Unknown model
}
````

### AI Service Abstraction

```go
type AIProvider interface {
    ProcessContent(ctx context.Context, req *ProcessingRequest) (*ProcessingResult, error)
    GetCapabilities() []string
    GetCost() float64
    GetLatency() time.Duration
    IsAvailable() bool
}

type AIServiceManager struct {
    providers map[string]AIProvider
    primary   string
    fallbacks []string
    logger    *slog.Logger
}

func NewAIServiceManager(config AIConfig) *AIServiceManager {
    manager := &AIServiceManager{
        providers: make(map[string]AIProvider),
        primary:   config.PrimaryProvider,
        fallbacks: config.FallbackProviders,
        logger:    slog.Default(),
    }

    // Initialize providers
    if config.OpenAI.Enabled {
        manager.providers["openai"] = NewOpenAIService(config.OpenAI.APIKey, config.OpenAI.Models)
    }

    if config.Ollama.Enabled {
        manager.providers["ollama"] = NewOllamaService(config.Ollama.Endpoint)
    }

    return manager
}

func (m *AIServiceManager) ProcessContent(ctx context.Context, req *ProcessingRequest) (*ProcessingResult, error) {
    // Try primary provider first
    if provider, exists := m.providers[m.primary]; exists && provider.IsAvailable() {
        result, err := provider.ProcessContent(ctx, req)
        if err == nil {
            return result, nil
        }

        m.logger.Warn("Primary provider failed", "provider", m.primary, "error", err)
    }

    // Try fallback providers
    for _, fallbackName := range m.fallbacks {
        if provider, exists := m.providers[fallbackName]; exists && provider.IsAvailable() {
            m.logger.Info("Using fallback provider", "provider", fallbackName)

            result, err := provider.ProcessContent(ctx, req)
            if err == nil {
                return result, nil
            }

            m.logger.Warn("Fallback provider failed", "provider", fallbackName, "error", err)
        }
    }

    return nil, fmt.Errorf("all AI providers failed")
}
```

### Circuit Breaker Implementation

```go
type CircuitBreakerState int

const (
    StateClosed CircuitBreakerState = iota
    StateOpen
    StateHalfOpen
)

type CircuitBreaker struct {
    failureThreshold int
    recoveryTimeout  time.Duration
    failureCount     int
    lastFailureTime  time.Time
    state           CircuitBreakerState
    mutex           sync.RWMutex
}

func (cb *CircuitBreaker) Execute(operation func() (any, error)) (any, error) {
    cb.mutex.Lock()

    if cb.state == StateOpen {
        if time.Since(cb.lastFailureTime) > cb.recoveryTimeout {
            cb.state = StateHalfOpen
            cb.failureCount = 0
        } else {
            cb.mutex.Unlock()
            return nil, ErrCircuitBreakerOpen
        }
    }

    cb.mutex.Unlock()

    result, err := operation()

    cb.mutex.Lock()
    defer cb.mutex.Unlock()

    if err != nil {
        cb.failureCount++
        cb.lastFailureTime = time.Now()

        if cb.failureCount >= cb.failureThreshold {
            cb.state = StateOpen
        }

        return nil, err
    }

    // Success - reset circuit breaker
    if cb.state == StateHalfOpen {
        cb.state = StateClosed
    }
    cb.failureCount = 0

    return result, nil
}
```

## Testing Strategy

### Unit Tests

- OpenAI client initialization and configuration
- Prompt building and response parsing
- Error handling and retry logic
- Circuit breaker state transitions
- Cost calculation accuracy

### Integration Tests

- End-to-end OpenAI API communication
- Fallback provider switching
- Cache hit/miss scenarios
- Rate limiting behavior
- Real API response parsing

### Mock Testing

- OpenAI API response simulation
- Error condition testing
- Performance testing with mocked responses
- Cost calculation validation
- Circuit breaker behavior

### Load Testing

- High-volume API requests
- Rate limiting effectiveness
- Cache performance under load
- Cost monitoring accuracy
- Error handling under stress

## Configuration

### Environment Variables

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-...
OPENAI_PRIMARY_MODEL=gpt-4-turbo
OPENAI_FALLBACK_MODEL=gpt-3.5-turbo
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.3

# Rate Limiting
OPENAI_REQUESTS_PER_MINUTE=50
OPENAI_DAILY_COST_LIMIT=100.00

# Circuit Breaker
OPENAI_FAILURE_THRESHOLD=5
OPENAI_RECOVERY_TIMEOUT=30s

# Caching
AI_CACHE_TTL=1h
AI_CACHE_MAX_SIZE=10000
```

### Configuration Structure

```go
type AIConfig struct {
    PrimaryProvider   string            `yaml:"primary_provider"`
    FallbackProviders []string          `yaml:"fallback_providers"`
    OpenAI           OpenAIConfig      `yaml:"openai"`
    Ollama           OllamaConfig      `yaml:"ollama"`
    Cache            CacheConfig       `yaml:"cache"`
    RateLimit        RateLimitConfig   `yaml:"rate_limit"`
}

type OpenAIConfig struct {
    Enabled      bool          `yaml:"enabled"`
    APIKey       string        `yaml:"api_key"`
    Models       ModelConfig   `yaml:"models"`
    DailyCostLimit float64     `yaml:"daily_cost_limit"`
}
```

## Monitoring and Metrics

### Prometheus Metrics

```go
var (
    APICalls = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "englog_ai_api_calls_total",
            Help: "Total number of AI API calls",
        },
        []string{"provider", "model", "status"},
    )

    TokensUsed = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "englog_ai_tokens_used_total",
            Help: "Total number of tokens used",
        },
        []string{"provider", "model"},
    )

    CostTotal = prometheus.NewCounter(
        prometheus.CounterOpts{
            Name: "englog_ai_cost_usd_total",
            Help: "Total cost in USD for AI processing",
        },
    )

    ProcessingDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "englog_ai_processing_duration_seconds",
            Help: "Time spent processing AI requests",
        },
        []string{"provider", "model"},
    )
)
```

### Cost Monitoring

- Daily cost tracking and alerts
- Usage quota monitoring
- Cost per request analysis
- Budget threshold notifications
- Token usage optimization

## Migration from Phase 0

### API Compatibility

- Maintain existing processing result format
- Preserve response structure
- Keep error codes consistent
- Ensure backward compatibility

### Configuration Migration

- Migrate from Ollama to OpenAI as primary
- Keep Ollama as fallback option
- Update environment variables
- Migrate existing prompts

## Dependencies

### External Dependencies

- OpenAI Go SDK
- Redis for caching
- Prometheus for metrics
- Rate limiting libraries

### Internal Dependencies

- gRPC Worker Pool (MVP-009)
- Redis Infrastructure
- Monitoring and metrics system
- Configuration management

## Deliverables

1. **OpenAI Client Service:** Complete OpenAI integration
2. **AI Service Manager:** Provider abstraction and switching
3. **Circuit Breaker:** Resilience and fault tolerance
4. **Caching Layer:** Response caching for cost optimization
5. **Cost Monitoring:** Usage tracking and budget controls
6. **Configuration:** Environment-based configuration system

## Definition of Done

- [ ] OpenAI GPT-4 integration functional
- [ ] Sentiment analysis accuracy >85% on test cases
- [ ] Response parsing handles all OpenAI formats
- [ ] Rate limiting prevents quota exhaustion
- [ ] Circuit breaker protects against failures
- [ ] Caching reduces API costs by >30%
- [ ] Cost monitoring tracks usage accurately
- [ ] Fallback to Ollama works seamlessly
- [ ] Unit tests achieve 90%+ coverage
- [ ] Integration tests with real API pass
- [ ] Load testing meets performance requirements
- [ ] Documentation completed

---

**Estimated Timeline:** 3 days
**Risk Level:** Medium (external API dependency)
**Blockers:** MVP-009 (gRPC Worker Pool required)
**Follow-up Tasks:** MVP-012 (Async AI Processing Pipeline)
