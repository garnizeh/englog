# MVP-009: gRPC Worker Pool Architecture

**Feature:** MVP-AI
**Priority:** P0 (Critical)
**Effort:** Large (5 days)
**Dependencies:** MVP-002 (Microservices Architecture)

## Objective

Implement distributed worker pool architecture with gRPC coordination, enabling scalable AI processing and replacing the synchronous processing from Phase 0 prototype.

## Technical Scope

### Worker Pool Architecture

- Worker registration and lifecycle management
- Task assignment and load balancing
- Health monitoring and failure detection
- Horizontal scaling capabilities
- Task queue management and priority handling

### gRPC Communication

- Task assignment protocol
- Status reporting and progress updates
- Worker heartbeat and health checks
- Error handling and retry mechanisms
- Task timeout and cancellation

### Task Management

- Task creation and queuing
- Priority-based task assignment
- Task status tracking and updates
- Result collection and storage
- Failed task retry logic

## Acceptance Criteria

### Functional Requirements

- [ ] Workers can register and deregister with API service
- [ ] Task assignment working with load balancing
- [ ] Worker health monitoring and failure detection
- [ ] Task status tracking from creation to completion
- [ ] Multiple workers can process tasks concurrently

### Scalability Requirements

- [ ] Support for multiple worker instances
- [ ] Dynamic worker scaling (add/remove workers)
- [ ] Load balancing across available workers
- [ ] Task queue handling during worker unavailability
- [ ] Graceful shutdown without losing tasks

### Reliability Requirements

- [ ] Worker failure detection and recovery
- [ ] Task retry on worker failure
- [ ] Heartbeat mechanism for worker health
- [ ] Task timeout handling
- [ ] Dead letter queue for failed tasks

## Implementation Details

### gRPC Service Definition

```protobuf
syntax = "proto3";

package englog.worker.v1;

service WorkerCoordinator {
  // Worker lifecycle
  rpc RegisterWorker(WorkerRegistration) returns (RegistrationResponse);
  rpc Heartbeat(HeartbeatRequest) returns (HeartbeatResponse);
  rpc UnregisterWorker(WorkerID) returns (UnregisterResponse);

  // Task management
  rpc GetTask(TaskRequest) returns (TaskAssignment);
  rpc ReportTaskStart(TaskStatusUpdate) returns (StatusResponse);
  rpc ReportTaskProgress(TaskProgressUpdate) returns (StatusResponse);
  rpc ReportTaskComplete(TaskCompletion) returns (StatusResponse);
  rpc ReportTaskError(TaskError) returns (StatusResponse);

  // Health and monitoring
  rpc GetWorkerStats(WorkerStatsRequest) returns (WorkerStatsResponse);
  rpc GetQueueStatus(QueueStatusRequest) returns (QueueStatusResponse);
}

message WorkerRegistration {
  string worker_id = 1;
  repeated string capabilities = 2;
  int32 max_concurrent_tasks = 3;
  WorkerResourceInfo resources = 4;
}

message TaskAssignment {
  string task_id = 1;
  string task_type = 2;
  string journal_id = 3;
  string user_id = 4;
  string content = 5;
  map<string, string> metadata = 6;
  int32 priority = 7;
  google.protobuf.Timestamp deadline = 8;
}

message TaskCompletion {
  string task_id = 1;
  string worker_id = 2;
  bool success = 3;
  string result_data = 4;
  string error_message = 5;
  google.protobuf.Timestamp completed_at = 6;
  TaskMetrics metrics = 7;
}
```

### Worker Pool Manager

```go
type WorkerPoolManager struct {
    workers       map[string]*WorkerInfo
    taskQueue     *TaskQueue
    assignmentMux sync.RWMutex
    logger        *slog.Logger
    metrics       *PoolMetrics
}

type WorkerInfo struct {
    ID             string
    Capabilities   []string
    MaxConcurrent  int
    CurrentTasks   int
    LastHeartbeat  time.Time
    Status         WorkerStatus
    ResourceUsage  ResourceInfo
}

type TaskQueue struct {
    pending     []*Task
    inProgress  map[string]*Task
    completed   map[string]*TaskResult
    failed      map[string]*TaskError
    mu          sync.RWMutex
    notify      chan struct{}
}

func (wm *WorkerPoolManager) RegisterWorker(ctx context.Context, req *WorkerRegistration) (*RegistrationResponse, error) {
    worker := &WorkerInfo{
        ID:            req.WorkerId,
        Capabilities:  req.Capabilities,
        MaxConcurrent: int(req.MaxConcurrentTasks),
        CurrentTasks:  0,
        LastHeartbeat: time.Now(),
        Status:        WorkerStatusActive,
    }

    wm.assignmentMux.Lock()
    wm.workers[worker.ID] = worker
    wm.assignmentMux.Unlock()

    wm.logger.Info("Worker registered", "worker_id", worker.ID, "capabilities", worker.Capabilities)

    return &RegistrationResponse{
        Success:      true,
        AssignedId:   worker.ID,
        PollInterval: durationpb.New(30 * time.Second),
    }, nil
}

func (wm *WorkerPoolManager) AssignTask(ctx context.Context, req *TaskRequest) (*TaskAssignment, error) {
    worker := wm.selectWorker(req.Capabilities)
    if worker == nil {
        return &TaskAssignment{}, status.Errorf(codes.ResourceExhausted, "No available workers")
    }

    task := wm.taskQueue.GetNextTask(worker.Capabilities)
    if task == nil {
        return &TaskAssignment{}, status.Errorf(codes.NotFound, "No tasks available")
    }

    // Assign task to worker
    worker.CurrentTasks++
    wm.taskQueue.MarkInProgress(task.ID, worker.ID)

    return &TaskAssignment{
        TaskId:    task.ID,
        TaskType:  task.Type,
        JournalId: task.JournalID,
        UserId:    task.UserID,
        Content:   task.Content,
        Metadata:  task.Metadata,
        Priority:  int32(task.Priority),
        Deadline:  timestamppb.New(task.Deadline),
    }, nil
}

func (wm *WorkerPoolManager) selectWorker(requiredCapabilities []string) *WorkerInfo {
    wm.assignmentMux.RLock()
    defer wm.assignmentMux.RUnlock()

    var bestWorker *WorkerInfo
    bestScore := -1

    for _, worker := range wm.workers {
        if worker.Status != WorkerStatusActive {
            continue
        }

        if worker.CurrentTasks >= worker.MaxConcurrent {
            continue
        }

        if !hasCapabilities(worker.Capabilities, requiredCapabilities) {
            continue
        }

        // Score based on current load (lower is better)
        score := worker.MaxConcurrent - worker.CurrentTasks
        if score > bestScore {
            bestScore = score
            bestWorker = worker
        }
    }

    return bestWorker
}
```

### Worker Client Implementation

```go
type WorkerClient struct {
    id           string
    capabilities []string
    maxConcurrent int
    currentTasks  int
    client       WorkerCoordinatorClient
    logger       *slog.Logger
    shutdown     chan struct{}
    wg           sync.WaitGroup
}

func (w *WorkerClient) Start(ctx context.Context) error {
    // Register with coordinator
    resp, err := w.client.RegisterWorker(ctx, &WorkerRegistration{
        WorkerId:           w.id,
        Capabilities:       w.capabilities,
        MaxConcurrentTasks: int32(w.maxConcurrent),
        Resources: &WorkerResourceInfo{
            CpuCores:    int32(runtime.NumCPU()),
            MemoryMb:    int32(getMemoryMB()),
            GpuAvailable: hasGPU(),
        },
    })
    if err != nil {
        return fmt.Errorf("failed to register worker: %w", err)
    }

    w.logger.Info("Worker registered successfully", "assigned_id", resp.AssignedId)

    // Start worker routines
    w.wg.Add(3)
    go w.heartbeatLoop(ctx)
    go w.taskPollingLoop(ctx)
    go w.healthMonitorLoop(ctx)

    return nil
}

func (w *WorkerClient) taskPollingLoop(ctx context.Context) {
    defer w.wg.Done()

    ticker := time.NewTicker(5 * time.Second)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return
        case <-w.shutdown:
            return
        case <-ticker.C:
            if w.currentTasks < w.maxConcurrent {
                w.pollForTask(ctx)
            }
        }
    }
}

func (w *WorkerClient) pollForTask(ctx context.Context) {
    task, err := w.client.GetTask(ctx, &TaskRequest{
        WorkerId:     w.id,
        Capabilities: w.capabilities,
    })
    if err != nil {
        if status.Code(err) != codes.NotFound {
            w.logger.Error("Failed to get task", "error", err)
        }
        return
    }

    // Process task in goroutine
    go w.processTask(ctx, task)
}

func (w *WorkerClient) processTask(ctx context.Context, task *TaskAssignment) {
    w.currentTasks++
    defer func() { w.currentTasks-- }()

    // Report task start
    w.client.ReportTaskStart(ctx, &TaskStatusUpdate{
        TaskId:    task.TaskId,
        WorkerId:  w.id,
        Status:    TaskStatus_PROCESSING,
        StartedAt: timestamppb.Now(),
    })

    // Process the task
    result, err := w.processAITask(ctx, task)
    if err != nil {
        w.client.ReportTaskError(ctx, &TaskError{
            TaskId:       task.TaskId,
            WorkerId:     w.id,
            ErrorMessage: err.Error(),
            ErrorType:    getErrorType(err),
            FailedAt:     timestamppb.Now(),
        })
        return
    }

    // Report completion
    w.client.ReportTaskComplete(ctx, &TaskCompletion{
        TaskId:      task.TaskId,
        WorkerId:    w.id,
        Success:     true,
        ResultData:  result,
        CompletedAt: timestamppb.Now(),
        Metrics: &TaskMetrics{
            ProcessingTimeMs: int64(time.Since(task.StartTime).Milliseconds()),
            MemoryUsedMb:     int64(getCurrentMemoryMB()),
        },
    })
}
```

### Task Queue Implementation

```go
type TaskQueue struct {
    pending     *PriorityQueue
    inProgress  map[string]*InProgressTask
    completed   map[string]*CompletedTask
    mu          sync.RWMutex
    notify      chan struct{}
    maxRetries  int
}

type Task struct {
    ID        string
    Type      string
    JournalID string
    UserID    string
    Content   string
    Metadata  map[string]string
    Priority  int
    Deadline  time.Time
    CreatedAt time.Time
    Retries   int
}

type InProgressTask struct {
    Task     *Task
    WorkerID string
    StartedAt time.Time
}

func (tq *TaskQueue) EnqueueTask(task *Task) error {
    tq.mu.Lock()
    defer tq.mu.Unlock()

    heap.Push(tq.pending, task)

    select {
    case tq.notify <- struct{}{}:
    default:
    }

    return nil
}

func (tq *TaskQueue) GetNextTask(workerCapabilities []string) *Task {
    tq.mu.Lock()
    defer tq.mu.Unlock()

    // Find first task that matches worker capabilities
    for i := 0; i < tq.pending.Len(); i++ {
        task := (*tq.pending)[i].(*Task)
        if tq.canWorkerProcess(task, workerCapabilities) {
            heap.Remove(tq.pending, i)
            return task
        }
    }

    return nil
}

func (tq *TaskQueue) MarkInProgress(taskID, workerID string) {
    tq.mu.Lock()
    defer tq.mu.Unlock()

    if task, exists := tq.inProgress[taskID]; exists {
        task.WorkerID = workerID
        task.StartedAt = time.Now()
    }
}

func (tq *TaskQueue) CompleteTask(taskID string, result *TaskCompletion) error {
    tq.mu.Lock()
    defer tq.mu.Unlock()

    inProgressTask, exists := tq.inProgress[taskID]
    if !exists {
        return fmt.Errorf("task not found in progress: %s", taskID)
    }

    // Move to completed
    tq.completed[taskID] = &CompletedTask{
        Task:        inProgressTask.Task,
        WorkerID:    inProgressTask.WorkerID,
        Result:      result.ResultData,
        CompletedAt: result.CompletedAt.AsTime(),
        Success:     result.Success,
    }

    delete(tq.inProgress, taskID)
    return nil
}
```

## Testing Strategy

### Unit Tests

- Worker registration and deregistration
- Task assignment logic and load balancing
- Task queue operations (enqueue, dequeue, priority)
- Heartbeat and health monitoring
- Error handling and retry logic

### Integration Tests

- End-to-end task processing flow
- Multiple workers handling concurrent tasks
- Worker failure scenarios and recovery
- Task timeout and cancellation
- gRPC communication reliability

### Load Tests

- High task throughput with multiple workers
- Worker scaling under load
- Queue performance with large task volumes
- Memory and CPU usage monitoring
- Task processing latency measurements

## Configuration

### Environment Variables

```env
# Worker Pool Configuration
WORKER_POOL_MAX_SIZE=10
WORKER_POLL_INTERVAL=5s
WORKER_HEARTBEAT_INTERVAL=30s
WORKER_HEALTH_CHECK_TIMEOUT=10s

# Task Configuration
TASK_DEFAULT_TIMEOUT=60s
TASK_MAX_RETRIES=3
TASK_QUEUE_MAX_SIZE=1000
TASK_PRIORITY_LEVELS=5

# gRPC Configuration
GRPC_SERVER_PORT=50051
GRPC_CLIENT_TIMEOUT=30s
GRPC_MAX_MESSAGE_SIZE=10MB
GRPC_KEEPALIVE_TIME=30s
```

### Docker Compose Integration

```yaml
services:
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    environment:
      - API_GRPC_ADDRESS=api:50051
      - WORKER_ID=worker-${RANDOM_ID}
      - MAX_CONCURRENT_TASKS=3
    deploy:
      replicas: 3
    depends_on:
      - api
```

## Monitoring and Observability

### Metrics Collection

```go
type PoolMetrics struct {
    ActiveWorkers      prometheus.Gauge
    QueuedTasks        prometheus.Gauge
    ProcessingTasks    prometheus.Gauge
    CompletedTasks     prometheus.Counter
    FailedTasks        prometheus.Counter
    TaskProcessingTime prometheus.Histogram
    WorkerUtilization  prometheus.Histogram
}

// Example metrics
var (
    ActiveWorkers = prometheus.NewGauge(prometheus.GaugeOpts{
        Name: "englog_active_workers_total",
        Help: "Number of active worker instances",
    })

    TaskProcessingTime = prometheus.NewHistogramVec(prometheus.HistogramOpts{
        Name: "englog_task_processing_duration_seconds",
        Help: "Time spent processing tasks",
    }, []string{"task_type", "worker_id"})
)
```

### Health Checks

- Worker pool health endpoint
- Individual worker status monitoring
- Task queue depth monitoring
- Processing time tracking
- Error rate monitoring

## Migration from Phase 0

### Architecture Changes

1. **Async Processing:** Replace synchronous AI calls with task queuing
2. **Worker Coordination:** Add gRPC communication layer
3. **Scalability:** Support multiple worker instances
4. **Reliability:** Add retry logic and error handling
5. **Monitoring:** Comprehensive observability

### Compatibility

- Maintain existing API response format
- Preserve processing result structure
- Keep journal creation flow consistent
- Ensure backward compatibility of data

## Dependencies

### External Dependencies

- gRPC Go libraries
- Protocol Buffer compiler
- Container orchestration (Docker Compose)
- Monitoring libraries (Prometheus)

### Internal Dependencies

- Microservices architecture (MVP-002)
- Database for task persistence
- AI processing services
- Logging and monitoring infrastructure

## Deliverables

1. **gRPC Service:** Complete worker coordination service
2. **Worker Pool Manager:** Task assignment and load balancing
3. **Worker Client:** Worker implementation with task processing
4. **Task Queue:** Priority-based task management system
5. **Monitoring:** Metrics and health check implementation
6. **Documentation:** Architecture and deployment documentation

## Definition of Done

- [ ] Worker registration and lifecycle management working
- [ ] Task assignment with load balancing functional
- [ ] Multiple workers processing tasks concurrently
- [ ] Worker health monitoring and failure recovery operational
- [ ] Task queue with priority handling implemented
- [ ] gRPC communication stable and reliable
- [ ] Monitoring and metrics collection active
- [ ] Unit and integration tests passing (85%+ coverage)
- [ ] Load testing completed with performance benchmarks
- [ ] Documentation completed with deployment instructions
- [ ] Code review completed and approved

---

**Estimated Timeline:** 5 days
**Risk Level:** High (complex distributed system coordination)
**Blockers:** MVP-002 (Microservices Architecture required)
**Follow-up Tasks:** MVP-010 (OpenAI Integration), MVP-012 (Async AI Processing Pipeline)
