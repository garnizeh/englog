# MVP-005: Journal Service Layer

**Feature:** MVP-Backend
**Priority:** P0 (Critical)
**Effort:** Medium (3 days)
**Dependencies:** MVP-004 (Journal Data Models)

## Objective

Implement the business logic layer for journal management, providing clean separation between HTTP handlers and data persistence with comprehensive journal operations, caching, and business rule enforcement.

## Technical Scope

### Service Layer Architecture

- Journal business logic implementation
- Service-level validation and rules
- Caching integration for performance
- Event publishing for async processing
- Transaction management for data consistency

### Core Operations

- Journal creation with metadata enrichment
- Retrieval with pagination and filtering
- Update operations with conflict resolution
- Deletion with safety checks
- Search functionality with ranking
- Batch operations for efficiency

### Business Rules

- User data isolation and security
- Processing status management
- Content analysis triggering
- Duplicate detection and handling
- Rate limiting and quota enforcement

## Acceptance Criteria

### Service Operations Requirements

- [ ] Create journal entries with automatic metadata enrichment
- [ ] Retrieve user journals with pagination (20 items per page)
- [ ] Update existing journals with conflict detection
- [ ] Delete journals with cascade to processed content
- [ ] Search journals with full-text search and relevance ranking

### Performance Requirements

- [ ] Service operations complete under 200ms for 95% of requests
- [ ] Caching reduces database queries by 60%+
- [ ] Batch operations handle 100+ entries efficiently
- [ ] Search results returned under 500ms
- [ ] Memory usage optimized for concurrent operations

### Business Logic Requirements

- [ ] User data isolation enforced at service level
- [ ] Processing pipeline triggered on journal creation
- [ ] Invalid operations rejected with clear error messages
- [ ] Audit logging for all journal operations
- [ ] Rate limiting prevents abuse

## Implementation Details

### Journal Service Interface

```go
type JournalService interface {
    // Core CRUD operations
    CreateJournal(ctx context.Context, userID string, req *CreateJournalRequest) (*JournalEntry, error)
    GetJournal(ctx context.Context, userID, journalID string) (*JournalEntry, error)
    GetUserJournals(ctx context.Context, userID string, options *GetJournalsOptions) (*JournalPage, error)
    UpdateJournal(ctx context.Context, userID, journalID string, req *UpdateJournalRequest) (*JournalEntry, error)
    DeleteJournal(ctx context.Context, userID, journalID string) error

    // Search and analytics
    SearchJournals(ctx context.Context, userID, query string, options *SearchOptions) (*JournalPage, error)
    GetJournalsByDateRange(ctx context.Context, userID string, start, end time.Time) ([]*JournalEntry, error)
    GetJournalStats(ctx context.Context, userID string) (*JournalStats, error)

    // Batch operations
    CreateJournals(ctx context.Context, userID string, requests []*CreateJournalRequest) ([]*JournalEntry, error)
    DeleteJournals(ctx context.Context, userID string, journalIDs []string) error

    // Processing management
    TriggerProcessing(ctx context.Context, journalID string) error
    GetProcessingStatus(ctx context.Context, journalID string) (*ProcessingStatus, error)
}

type JournalServiceImpl struct {
    repo           JournalRepository
    userRepo       UserRepository
    validator      *Validator
    cache          CacheService
    eventPublisher EventPublisher
    logger         *slog.Logger
    metrics        *ServiceMetrics
    config         JournalServiceConfig
}

type JournalServiceConfig struct {
    MaxJournalsPerUser     int           `env:"MAX_JOURNALS_PER_USER" default:"10000"`
    DefaultPageSize        int           `env:"DEFAULT_PAGE_SIZE" default:"20"`
    MaxPageSize            int           `env:"MAX_PAGE_SIZE" default:"100"`
    CacheTTL               time.Duration `env:"CACHE_TTL" default:"15m"`
    SearchCacheTTL         time.Duration `env:"SEARCH_CACHE_TTL" default:"5m"`
    EnableAutoProcessing   bool          `env:"ENABLE_AUTO_PROCESSING" default:"true"`
    MaxConcurrentBatchSize int           `env:"MAX_CONCURRENT_BATCH_SIZE" default:"50"`
}

type GetJournalsOptions struct {
    Page         int               `json:"page" validate:"min=1"`
    PageSize     int               `json:"page_size" validate:"min=1,max=100"`
    SortBy       string            `json:"sort_by" validate:"oneof=created_at updated_at"`
    SortOrder    string            `json:"sort_order" validate:"oneof=asc desc"`
    EntryType    *string           `json:"entry_type,omitempty"`
    Status       *ProcessingStatus `json:"status,omitempty"`
    DateFrom     *time.Time        `json:"date_from,omitempty"`
    DateTo       *time.Time        `json:"date_to,omitempty"`
    IncludeStats bool              `json:"include_stats"`
}

type SearchOptions struct {
    Page      int    `json:"page" validate:"min=1"`
    PageSize  int    `json:"page_size" validate:"min=1,max=100"`
    SortBy    string `json:"sort_by" validate:"oneof=relevance created_at"`
    Highlight bool   `json:"highlight"`
}

type JournalPage struct {
    Journals   []*JournalEntry `json:"journals"`
    Pagination PaginationInfo  `json:"pagination"`
    Stats      *JournalStats   `json:"stats,omitempty"`
}

type PaginationInfo struct {
    Page         int  `json:"page"`
    PageSize     int  `json:"page_size"`
    TotalPages   int  `json:"total_pages"`
    TotalCount   int  `json:"total_count"`
    HasPrevious  bool `json:"has_previous"`
    HasNext      bool `json:"has_next"`
}

type JournalStats struct {
    TotalJournals     int                        `json:"total_journals"`
    EntriesByType     map[string]int             `json:"entries_by_type"`
    EntriesByStatus   map[ProcessingStatus]int   `json:"entries_by_status"`
    WordCount         int                        `json:"total_word_count"`
    AvgWordsPerEntry  float64                    `json:"avg_words_per_entry"`
    OldestEntry       *time.Time                 `json:"oldest_entry"`
    NewestEntry       *time.Time                 `json:"newest_entry"`
    EntriesThisWeek   int                        `json:"entries_this_week"`
    EntriesThisMonth  int                        `json:"entries_this_month"`
}

func NewJournalService(
    repo JournalRepository,
    userRepo UserRepository,
    validator *Validator,
    cache CacheService,
    eventPublisher EventPublisher,
    logger *slog.Logger,
    config JournalServiceConfig,
) *JournalServiceImpl {
    return &JournalServiceImpl{
        repo:           repo,
        userRepo:       userRepo,
        validator:      validator,
        cache:          cache,
        eventPublisher: eventPublisher,
        logger:         logger,
        config:         config,
        metrics:        NewServiceMetrics(),
    }
}
```

### Core Service Operations

```go
func (s *JournalServiceImpl) CreateJournal(ctx context.Context, userID string, req *CreateJournalRequest) (*JournalEntry, error) {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("create").Observe(time.Since(start).Seconds())
    }()

    s.logger.Info("Creating journal entry",
        "user_id", userID,
        "entry_type", req.EntryType,
        "content_length", len(req.Content))

    // Validate request
    if err := s.validator.ValidateJournalRequest(req); err != nil {
        s.metrics.ValidationErrors.Inc()
        return nil, fmt.Errorf("validation failed: %w", err)
    }

    // Sanitize input
    s.validator.SanitizeJournalRequest(req)

    // Check user exists and get preferences
    user, err := s.userRepo.GetByID(ctx, userID)
    if err != nil {
        return nil, fmt.Errorf("failed to get user: %w", err)
    }

    // Check user journal quota
    if err := s.checkUserQuota(ctx, userID); err != nil {
        return nil, err
    }

    // Enrich metadata
    metadata := s.enrichMetadata(req, user)

    // Create journal entry
    journal := &JournalEntry{
        ID:               generateJournalID(),
        UserID:           userID,
        RawData:          s.buildRawData(req),
        EntryType:        req.EntryType,
        ProcessingStatus: string(ProcessingStatusPending),
        Metadata:         metadata,
        CreatedAt:        time.Now(),
        UpdatedAt:        time.Now(),
    }

    // Start database transaction
    tx, err := s.repo.BeginTx(ctx)
    if err != nil {
        return nil, fmt.Errorf("failed to start transaction: %w", err)
    }
    defer tx.Rollback()

    // Save journal to database
    if err := s.repo.CreateWithTx(ctx, tx, journal); err != nil {
        s.metrics.DatabaseErrors.Inc()
        return nil, fmt.Errorf("failed to create journal: %w", err)
    }

    // Commit transaction
    if err := tx.Commit(); err != nil {
        return nil, fmt.Errorf("failed to commit transaction: %w", err)
    }

    // Cache the journal
    s.cacheJournal(ctx, journal)

    // Invalidate user journals cache
    s.invalidateUserJournalsCache(userID)

    // Publish creation event for async processing
    if s.config.EnableAutoProcessing {
        event := &JournalCreatedEvent{
            JournalID: journal.ID,
            UserID:    userID,
            EntryType: journal.EntryType,
            CreatedAt: journal.CreatedAt,
        }

        if err := s.eventPublisher.PublishJournalCreated(ctx, event); err != nil {
            s.logger.Warn("Failed to publish journal created event",
                "journal_id", journal.ID,
                "error", err)
        }
    }

    s.metrics.JournalsCreated.Inc()
    s.logger.Info("Journal entry created successfully",
        "journal_id", journal.ID,
        "user_id", userID)

    return journal, nil
}

func (s *JournalServiceImpl) GetJournal(ctx context.Context, userID, journalID string) (*JournalEntry, error) {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("get").Observe(time.Since(start).Seconds())
    }()

    // Try cache first
    cacheKey := s.buildJournalCacheKey(journalID)
    var journal JournalEntry

    if err := s.cache.Get(ctx, cacheKey, &journal); err == nil {
        // Verify user ownership
        if journal.UserID != userID {
            return nil, ErrJournalNotFound
        }
        s.metrics.CacheHits.Inc()
        return &journal, nil
    }

    s.metrics.CacheMisses.Inc()

    // Fetch from database
    journalPtr, err := s.repo.GetByID(ctx, journalID)
    if err != nil {
        if err == ErrJournalNotFound {
            return nil, err
        }
        s.metrics.DatabaseErrors.Inc()
        return nil, fmt.Errorf("failed to get journal: %w", err)
    }

    // Verify user ownership
    if journalPtr.UserID != userID {
        return nil, ErrJournalNotFound
    }

    // Cache the result
    s.cacheJournal(ctx, journalPtr)

    return journalPtr, nil
}

func (s *JournalServiceImpl) GetUserJournals(ctx context.Context, userID string, options *GetJournalsOptions) (*JournalPage, error) {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("list").Observe(time.Since(start).Seconds())
    }()

    // Validate and set defaults
    if options == nil {
        options = &GetJournalsOptions{}
    }
    s.setDefaultOptions(options)

    if err := s.validator.validate.Struct(options); err != nil {
        return nil, fmt.Errorf("invalid options: %w", err)
    }

    // Try cache for frequently accessed first page
    if options.Page == 1 && options.PageSize == s.config.DefaultPageSize {
        cacheKey := s.buildUserJournalsCacheKey(userID, options)
        var page JournalPage

        if err := s.cache.Get(ctx, cacheKey, &page); err == nil {
            s.metrics.CacheHits.Inc()
            return &page, nil
        }
    }

    s.metrics.CacheMisses.Inc()

    // Calculate offset
    offset := (options.Page - 1) * options.PageSize

    // Get journals from repository
    journals, err := s.repo.GetByUserIDWithOptions(ctx, userID, options.PageSize, offset, options)
    if err != nil {
        s.metrics.DatabaseErrors.Inc()
        return nil, fmt.Errorf("failed to get user journals: %w", err)
    }

    // Get total count for pagination
    totalCount, err := s.repo.CountByUserID(ctx, userID, options)
    if err != nil {
        s.logger.Warn("Failed to get total count", "error", err)
        totalCount = len(journals) // Fallback
    }

    // Build pagination info
    pagination := s.buildPaginationInfo(options.Page, options.PageSize, totalCount)

    // Get stats if requested
    var stats *JournalStats
    if options.IncludeStats {
        stats, _ = s.getJournalStatsInternal(ctx, userID)
    }

    page := &JournalPage{
        Journals:   journals,
        Pagination: pagination,
        Stats:      stats,
    }

    // Cache first page results
    if options.Page == 1 && options.PageSize == s.config.DefaultPageSize {
        cacheKey := s.buildUserJournalsCacheKey(userID, options)
        s.cache.Set(ctx, cacheKey, page, s.config.CacheTTL)
    }

    return page, nil
}

func (s *JournalServiceImpl) UpdateJournal(ctx context.Context, userID, journalID string, req *UpdateJournalRequest) (*JournalEntry, error) {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("update").Observe(time.Since(start).Seconds())
    }()

    s.logger.Info("Updating journal entry",
        "user_id", userID,
        "journal_id", journalID)

    // Validate request
    if err := s.validator.validate.Struct(req); err != nil {
        s.metrics.ValidationErrors.Inc()
        return nil, fmt.Errorf("validation failed: %w", err)
    }

    // Get existing journal
    journal, err := s.GetJournal(ctx, userID, journalID)
    if err != nil {
        return nil, err
    }

    // Check if journal is currently being processed
    if journal.ProcessingStatus == string(ProcessingStatusProcessing) {
        return nil, ErrJournalBeingProcessed
    }

    // Apply updates
    updated := false

    if req.Content != nil {
        // Sanitize content
        sanitized := s.validator.sanitizer.Sanitize(*req.Content)
        journal.RawData["content"] = sanitized

        // Update word count in metadata
        wordCount := s.calculateWordCount(sanitized)
        journal.Metadata.WordCount = wordCount
        updated = true
    }

    if req.Title != nil {
        sanitized := s.validator.sanitizer.Sanitize(*req.Title)
        if sanitized == "" {
            delete(journal.RawData, "title")
        } else {
            journal.RawData["title"] = sanitized
        }
        updated = true
    }

    if req.Tags != nil {
        journal.RawData["tags"] = req.Tags
        updated = true
    }

    if req.Mood != nil {
        journal.RawData["mood"] = *req.Mood
        updated = true
    }

    if req.Location != nil {
        journal.RawData["location"] = req.Location
        updated = true
    }

    if req.CustomFields != nil {
        // Validate custom fields
        if err := s.validator.validateCustomFields(req.CustomFields); err != nil {
            return nil, err
        }

        // Merge custom fields
        if existingFields, ok := journal.RawData["custom_fields"].(map[string]any); ok {
            for k, v := range req.CustomFields {
                existingFields[k] = v
            }
        } else {
            journal.RawData["custom_fields"] = req.CustomFields
        }
        updated = true
    }

    if req.PrivateNotes != nil {
        sanitized := s.validator.sanitizer.Sanitize(*req.PrivateNotes)
        if sanitized == "" {
            delete(journal.RawData, "private_notes")
        } else {
            journal.RawData["private_notes"] = sanitized
        }
        updated = true
    }

    if !updated {
        return journal, nil // No changes
    }

    // Reset processing status to trigger re-analysis
    journal.ProcessingStatus = string(ProcessingStatusPending)
    journal.UpdatedAt = time.Now()

    // Update in database
    if err := s.repo.Update(ctx, journal); err != nil {
        s.metrics.DatabaseErrors.Inc()
        return nil, fmt.Errorf("failed to update journal: %w", err)
    }

    // Update cache
    s.cacheJournal(ctx, journal)

    // Invalidate related caches
    s.invalidateUserJournalsCache(userID)

    // Publish update event for re-processing
    if s.config.EnableAutoProcessing {
        event := &JournalUpdatedEvent{
            JournalID: journal.ID,
            UserID:    userID,
            UpdatedAt: journal.UpdatedAt,
        }

        if err := s.eventPublisher.PublishJournalUpdated(ctx, event); err != nil {
            s.logger.Warn("Failed to publish journal updated event",
                "journal_id", journal.ID,
                "error", err)
        }
    }

    s.metrics.JournalsUpdated.Inc()
    s.logger.Info("Journal entry updated successfully",
        "journal_id", journal.ID,
        "user_id", userID)

    return journal, nil
}

func (s *JournalServiceImpl) DeleteJournal(ctx context.Context, userID, journalID string) error {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("delete").Observe(time.Since(start).Seconds())
    }()

    s.logger.Info("Deleting journal entry",
        "user_id", userID,
        "journal_id", journalID)

    // Verify journal exists and user ownership
    journal, err := s.GetJournal(ctx, userID, journalID)
    if err != nil {
        return err
    }

    // Start transaction for cascade delete
    tx, err := s.repo.BeginTx(ctx)
    if err != nil {
        return fmt.Errorf("failed to start transaction: %w", err)
    }
    defer tx.Rollback()

    // Delete journal entry (cascade will delete processed content)
    if err := s.repo.DeleteWithTx(ctx, tx, journalID); err != nil {
        s.metrics.DatabaseErrors.Inc()
        return fmt.Errorf("failed to delete journal: %w", err)
    }

    // Commit transaction
    if err := tx.Commit(); err != nil {
        return fmt.Errorf("failed to commit delete transaction: %w", err)
    }

    // Remove from cache
    cacheKey := s.buildJournalCacheKey(journalID)
    s.cache.Delete(ctx, cacheKey)

    // Invalidate user journals cache
    s.invalidateUserJournalsCache(userID)

    // Publish deletion event
    event := &JournalDeletedEvent{
        JournalID: journalID,
        UserID:    userID,
        DeletedAt: time.Now(),
    }

    if err := s.eventPublisher.PublishJournalDeleted(ctx, event); err != nil {
        s.logger.Warn("Failed to publish journal deleted event",
            "journal_id", journalID,
            "error", err)
    }

    s.metrics.JournalsDeleted.Inc()
    s.logger.Info("Journal entry deleted successfully",
        "journal_id", journalID,
        "user_id", userID)

    return nil
}
```

### Search and Analytics

```go
func (s *JournalServiceImpl) SearchJournals(ctx context.Context, userID, query string, options *SearchOptions) (*JournalPage, error) {
    start := time.Now()
    defer func() {
        s.metrics.OperationDuration.WithLabelValues("search").Observe(time.Since(start).Seconds())
    }()

    if options == nil {
        options = &SearchOptions{
            Page:     1,
            PageSize: s.config.DefaultPageSize,
            SortBy:   "relevance",
        }
    }

    if err := s.validator.validate.Struct(options); err != nil {
        return nil, fmt.Errorf("invalid search options: %w", err)
    }

    // Sanitize search query
    cleanQuery := s.sanitizeSearchQuery(query)
    if cleanQuery == "" {
        return &JournalPage{
            Journals:   []*JournalEntry{},
            Pagination: PaginationInfo{Page: 1, PageSize: options.PageSize},
        }, nil
    }

    // Try cache for common searches
    cacheKey := s.buildSearchCacheKey(userID, cleanQuery, options)
    var page JournalPage

    if err := s.cache.Get(ctx, cacheKey, &page); err == nil {
        s.metrics.CacheHits.Inc()
        return &page, nil
    }

    s.metrics.CacheMisses.Inc()

    // Calculate offset
    offset := (options.Page - 1) * options.PageSize

    // Perform search
    journals, err := s.repo.Search(ctx, userID, cleanQuery, options.PageSize, offset)
    if err != nil {
        s.metrics.DatabaseErrors.Inc()
        return nil, fmt.Errorf("failed to search journals: %w", err)
    }

    // Get total count for search results
    totalCount, err := s.repo.CountSearch(ctx, userID, cleanQuery)
    if err != nil {
        s.logger.Warn("Failed to get search count", "error", err)
        totalCount = len(journals)
    }

    // Build pagination
    pagination := s.buildPaginationInfo(options.Page, options.PageSize, totalCount)

    // Add search highlighting if requested
    if options.Highlight {
        s.addSearchHighlights(journals, cleanQuery)
    }

    searchPage := &JournalPage{
        Journals:   journals,
        Pagination: pagination,
    }

    // Cache search results
    s.cache.Set(ctx, cacheKey, searchPage, s.config.SearchCacheTTL)

    s.metrics.SearchQueries.Inc()
    return searchPage, nil
}

func (s *JournalServiceImpl) GetJournalStats(ctx context.Context, userID string) (*JournalStats, error) {
    // Try cache first
    cacheKey := s.buildStatsCacheKey(userID)
    var stats JournalStats

    if err := s.cache.Get(ctx, cacheKey, &stats); err == nil {
        s.metrics.CacheHits.Inc()
        return &stats, nil
    }

    return s.getJournalStatsInternal(ctx, userID)
}

func (s *JournalServiceImpl) getJournalStatsInternal(ctx context.Context, userID string) (*JournalStats, error) {
    s.metrics.CacheMisses.Inc()

    // Get basic stats from repository
    stats, err := s.repo.GetUserStats(ctx, userID)
    if err != nil {
        return nil, fmt.Errorf("failed to get journal stats: %w", err)
    }

    // Calculate time-based stats
    now := time.Now()
    weekStart := now.AddDate(0, 0, -7)
    monthStart := now.AddDate(0, -1, 0)

    thisWeekJournals, _ := s.repo.CountByDateRange(ctx, userID, weekStart, now)
    thisMonthJournals, _ := s.repo.CountByDateRange(ctx, userID, monthStart, now)

    stats.EntriesThisWeek = thisWeekJournals
    stats.EntriesThisMonth = thisMonthJournals

    // Cache stats
    cacheKey := s.buildStatsCacheKey(userID)
    s.cache.Set(ctx, cacheKey, stats, s.config.CacheTTL)

    return stats, nil
}
```

### Helper Functions

```go
func (s *JournalServiceImpl) enrichMetadata(req *CreateJournalRequest, user *User) JournalMetadata {
    metadata := JournalMetadata{
        Source:        "web_app", // Will be configurable
        WordCount:     s.calculateWordCount(req.Content),
        ReadingTime:   s.calculateReadingTime(req.Content),
        CustomFields:  make(map[string]any),
    }

    // Add location if provided
    if req.Location != nil {
        metadata.Location = req.Location
    }

    // Calculate reading time (average 200 words per minute)
    if metadata.WordCount > 0 {
        metadata.ReadingTime = int(math.Ceil(float64(metadata.WordCount) / 200.0))
    }

    return metadata
}

func (s *JournalServiceImpl) buildRawData(req *CreateJournalRequest) map[string]any {
    rawData := map[string]any{
        "content": req.Content,
    }

    if req.Title != nil && *req.Title != "" {
        rawData["title"] = *req.Title
    }

    if len(req.Tags) > 0 {
        rawData["tags"] = req.Tags
    }

    if req.Mood != nil {
        rawData["mood"] = *req.Mood
    }

    if req.Location != nil {
        rawData["location"] = req.Location
    }

    if len(req.CustomFields) > 0 {
        rawData["custom_fields"] = req.CustomFields
    }

    if req.PrivateNotes != "" {
        rawData["private_notes"] = req.PrivateNotes
    }

    return rawData
}

func (s *JournalServiceImpl) calculateWordCount(content string) int {
    words := strings.Fields(content)
    return len(words)
}

func (s *JournalServiceImpl) calculateReadingTime(content string) int {
    wordCount := s.calculateWordCount(content)
    if wordCount == 0 {
        return 0
    }
    return int(math.Ceil(float64(wordCount) / 200.0)) // 200 WPM average
}

func (s *JournalServiceImpl) checkUserQuota(ctx context.Context, userID string) error {
    count, err := s.repo.CountByUserID(ctx, userID, nil)
    if err != nil {
        return fmt.Errorf("failed to check user quota: %w", err)
    }

    if count >= s.config.MaxJournalsPerUser {
        return ErrUserQuotaExceeded
    }

    return nil
}

func (s *JournalServiceImpl) buildJournalCacheKey(journalID string) string {
    return fmt.Sprintf("journal:%s", journalID)
}

func (s *JournalServiceImpl) buildUserJournalsCacheKey(userID string, options *GetJournalsOptions) string {
    return fmt.Sprintf("user_journals:%s:page_%d:size_%d", userID, options.Page, options.PageSize)
}

func (s *JournalServiceImpl) buildSearchCacheKey(userID, query string, options *SearchOptions) string {
    return fmt.Sprintf("search:%s:%s:page_%d", userID, query, options.Page)
}

func (s *JournalServiceImpl) buildStatsCacheKey(userID string) string {
    return fmt.Sprintf("stats:%s", userID)
}

func (s *JournalServiceImpl) cacheJournal(ctx context.Context, journal *JournalEntry) {
    cacheKey := s.buildJournalCacheKey(journal.ID)
    if err := s.cache.Set(ctx, cacheKey, journal, s.config.CacheTTL); err != nil {
        s.logger.Warn("Failed to cache journal", "journal_id", journal.ID, "error", err)
    }
}

func (s *JournalServiceImpl) invalidateUserJournalsCache(userID string) {
    // This is a simplified approach - in production you might use cache tags
    pattern := fmt.Sprintf("user_journals:%s:*", userID)
    s.cache.DeletePattern(context.Background(), pattern)
}

func generateJournalID() string {
    return uuid.New().String()
}
```

## Testing Strategy

### Unit Tests

- Service method logic and validation
- Business rule enforcement
- Error handling scenarios
- Cache integration
- Event publishing

### Integration Tests

- End-to-end service operations
- Database transaction handling
- Cache consistency
- Event delivery
- Performance under load

### Mock Dependencies

- Repository layer mocking
- Cache service mocking
- Event publisher mocking
- User repository mocking

## Configuration

### Environment Variables

```env
# Service Configuration
MAX_JOURNALS_PER_USER=10000
DEFAULT_PAGE_SIZE=20
MAX_PAGE_SIZE=100
CACHE_TTL=15m
SEARCH_CACHE_TTL=5m
ENABLE_AUTO_PROCESSING=true
MAX_CONCURRENT_BATCH_SIZE=50

# Performance Settings
SERVICE_TIMEOUT=30s
MAX_CONCURRENT_OPERATIONS=100
CACHE_SIZE_MB=256
```

## Dependencies

### External Dependencies

- Repository layer (MVP-004)
- Cache service (Redis)
- Event publishing system
- Validation framework
- Metrics collection

### Internal Dependencies

- User repository for user validation
- Authentication context
- Logging infrastructure
- Configuration management

## Deliverables

1. **Service Interface:** Complete business logic layer
2. **Cache Integration:** Performance optimization with Redis
3. **Event Publishing:** Async processing triggers
4. **Business Rules:** User data isolation and validation
5. **Test Suite:** Comprehensive unit and integration tests
6. **Metrics:** Service performance monitoring

## Definition of Done

- [ ] All CRUD operations implemented with business logic
- [ ] User data isolation enforced at service level
- [ ] Caching reduces database queries by 60%+
- [ ] Search functionality works with full-text search
- [ ] Batch operations handle 100+ entries efficiently
- [ ] Service operations complete under 200ms for 95% of requests
- [ ] Event publishing triggers async processing
- [ ] Comprehensive error handling with clear messages
- [ ] Unit tests achieve 95%+ coverage
- [ ] Integration tests validate end-to-end flows
- [ ] Performance tests meet latency requirements
- [ ] Audit logging captures all operations
- [ ] Rate limiting prevents service abuse

---

**Estimated Timeline:** 3 days
**Risk Level:** Medium (business logic complexity)
**Blockers:** MVP-004 (Journal Data Models)
**Follow-up Tasks:** MVP-008 (Authenticated API), MVP-012 (Async Processing)
