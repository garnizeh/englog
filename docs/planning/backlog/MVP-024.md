# MVP-024: Monitoring & Observability

**Task ID:** MVP-024
**Feature Area:** MVP-DEVOPS
**Priority:** P1 (High)
**Effort Estimate:** 5 days
**Dependencies:** MVP-022 (Docker Compose Production Setup), MVP-023 (CI/CD Pipeline Implementation)

## Objective

Implement comprehensive monitoring and observability infrastructure with metrics collection, alerting, distributed tracing, and performance monitoring to ensure optimal system health and rapid issue detection in production.

## Business Context

Comprehensive monitoring is critical for maintaining high availability, detecting issues before they impact users, optimizing performance, and meeting SLA commitments. This enables proactive system management and data-driven optimization decisions.

## Technical Requirements

### Core Monitoring Components

1. **Metrics Collection**

   - Application metrics (business and technical)
   - Infrastructure metrics (CPU, memory, disk, network)
   - Custom metrics for AI processing and user behavior
   - Real-time metric aggregation and storage

2. **Alerting System**

   - Threshold-based alerts with escalation
   - Anomaly detection and smart alerting
   - Multi-channel notifications (Slack, email, PagerDuty)
   - Alert routing and acknowledgment

3. **Observability Platform**

   - Distributed tracing for request flow
   - Structured logging aggregation
   - Performance monitoring and APM
   - Real-time dashboards and visualization

4. **Health Monitoring**
   - Service health checks and availability
   - Dependency monitoring (database, Redis, AI services)
   - SLA monitoring and reporting
   - Automated recovery procedures

## Implementation Specification

### Prometheus Configuration

```yaml
# monitoring/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: "englog-production"
    region: "us-west-2"

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

scrape_configs:
  # API Service metrics
  - job_name: "englog-api"
    static_configs:
      - targets: ["api:8080"]
    metrics_path: "/metrics"
    scrape_interval: 10s
    scrape_timeout: 5s

  # Worker Service metrics
  - job_name: "englog-worker"
    static_configs:
      - targets: ["worker:8081"]
    metrics_path: "/metrics"
    scrape_interval: 15s

  # Database metrics
  - job_name: "postgres"
    static_configs:
      - targets: ["postgres-exporter:9187"]

  # Redis metrics
  - job_name: "redis"
    static_configs:
      - targets: ["redis-exporter:9121"]

  # Nginx metrics
  - job_name: "nginx"
    static_configs:
      - targets: ["nginx-exporter:9113"]

  # Node exporter for system metrics
  - job_name: "node"
    static_configs:
      - targets: ["node-exporter:9100"]

  # cAdvisor for container metrics
  - job_name: "cadvisor"
    static_configs:
      - targets: ["cadvisor:8080"]
    metrics_path: "/metrics"

  # Blackbox exporter for endpoint monitoring
  - job_name: "blackbox"
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - https://api.englog.com/health
          - https://englog.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
```

### Application Metrics Implementation

```go
// internal/metrics/metrics.go
package metrics

import (
    "time"

    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    "github.com/gin-gonic/gin"
)

var (
    // HTTP metrics
    HTTPRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "http",
            Name:      "requests_total",
            Help:      "Total number of HTTP requests",
        },
        []string{"method", "endpoint", "status_code"},
    )

    HTTPRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Namespace: "englog",
            Subsystem: "http",
            Name:      "request_duration_seconds",
            Help:      "HTTP request duration in seconds",
            Buckets:   prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )

    // Business metrics
    JournalsCreated = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "business",
            Name:      "journals_created_total",
            Help:      "Total number of journals created",
        },
        []string{"user_id", "entry_type"},
    )

    AIProcessingDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Namespace: "englog",
            Subsystem: "ai",
            Name:      "processing_duration_seconds",
            Help:      "AI processing duration in seconds",
            Buckets:   []float64{1, 5, 10, 30, 60, 120, 300},
        },
        []string{"task_type", "provider"},
    )

    AIProcessingErrors = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "ai",
            Name:      "processing_errors_total",
            Help:      "Total number of AI processing errors",
        },
        []string{"task_type", "provider", "error_type"},
    )

    // Database metrics
    DatabaseConnections = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Namespace: "englog",
            Subsystem: "database",
            Name:      "connections_active",
            Help:      "Number of active database connections",
        },
        []string{"database"},
    )

    DatabaseQueryDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Namespace: "englog",
            Subsystem: "database",
            Name:      "query_duration_seconds",
            Help:      "Database query duration in seconds",
            Buckets:   []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5},
        },
        []string{"operation", "table"},
    )

    // Cache metrics
    CacheHits = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "cache",
            Name:      "hits_total",
            Help:      "Total number of cache hits",
        },
        []string{"cache_type", "key_pattern"},
    )

    CacheMisses = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "cache",
            Name:      "misses_total",
            Help:      "Total number of cache misses",
        },
        []string{"cache_type", "key_pattern"},
    )

    // User behavior metrics
    ActiveUsers = promauto.NewGaugeVec(
        prometheus.GaugeOpts{
            Namespace: "englog",
            Subsystem: "users",
            Name:      "active_total",
            Help:      "Number of active users",
        },
        []string{"time_window"}, // 1h, 24h, 7d
    )

    UserSessions = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Namespace: "englog",
            Subsystem: "users",
            Name:      "sessions_total",
            Help:      "Total number of user sessions",
        },
        []string{"auth_method"},
    )
)

// Middleware for HTTP metrics
func PrometheusMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        start := time.Now()

        c.Next()

        duration := time.Since(start).Seconds()
        status := c.Writer.Status()

        HTTPRequestsTotal.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
            fmt.Sprintf("%d", status),
        ).Inc()

        HTTPRequestDuration.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
        ).Observe(duration)
    }
}

// Business metrics helpers
func RecordJournalCreated(userID, entryType string) {
    JournalsCreated.WithLabelValues(userID, entryType).Inc()
}

func RecordAIProcessing(taskType, provider string, duration time.Duration, err error) {
    AIProcessingDuration.WithLabelValues(taskType, provider).Observe(duration.Seconds())

    if err != nil {
        errorType := "unknown"
        if strings.Contains(err.Error(), "timeout") {
            errorType = "timeout"
        } else if strings.Contains(err.Error(), "rate_limit") {
            errorType = "rate_limit"
        }
        AIProcessingErrors.WithLabelValues(taskType, provider, errorType).Inc()
    }
}

func RecordDatabaseQuery(operation, table string, duration time.Duration) {
    DatabaseQueryDuration.WithLabelValues(operation, table).Observe(duration.Seconds())
}

func UpdateActiveUsers(count int, timeWindow string) {
    ActiveUsers.WithLabelValues(timeWindow).Set(float64(count))
}
```

### Grafana Dashboards

```json
{
  "dashboard": {
    "id": null,
    "title": "EngLog - System Overview",
    "tags": ["englog", "overview"],
    "timezone": "UTC",
    "panels": [
      {
        "id": 1,
        "title": "API Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(englog_http_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                { "color": "green", "value": null },
                { "color": "yellow", "value": 50 },
                { "color": "red", "value": 100 }
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "Response Time P95",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(englog_http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95 Response Time"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "steps": [
                { "color": "green", "value": null },
                { "color": "yellow", "value": 0.5 },
                { "color": "red", "value": 2 }
              ]
            }
          }
        }
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(englog_http_requests_total{status_code=~\"5..\"}[5m]) / rate(englog_http_requests_total[5m]) * 100",
            "legendFormat": "Error Rate %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                { "color": "green", "value": null },
                { "color": "yellow", "value": 1 },
                { "color": "red", "value": 5 }
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Active Users",
        "type": "timeseries",
        "targets": [
          {
            "expr": "englog_users_active_total",
            "legendFormat": "Active Users - {{time_window}}"
          }
        ]
      },
      {
        "id": 5,
        "title": "AI Processing Performance",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(englog_ai_processing_duration_seconds_bucket[5m]))",
            "legendFormat": "P95 AI Processing Time - {{task_type}}"
          }
        ]
      },
      {
        "id": 6,
        "title": "Database Performance",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(englog_database_query_duration_seconds_sum[5m]) / rate(englog_database_query_duration_seconds_count[5m])",
            "legendFormat": "Avg Query Time - {{operation}}"
          }
        ]
      },
      {
        "id": 7,
        "title": "System Resources",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total[5m]) * 100",
            "legendFormat": "CPU Usage % - {{name}}"
          },
          {
            "expr": "(container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100",
            "legendFormat": "Memory Usage % - {{name}}"
          }
        ]
      },
      {
        "id": 8,
        "title": "Cache Performance",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(englog_cache_hits_total[5m]) / (rate(englog_cache_hits_total[5m]) + rate(englog_cache_misses_total[5m])) * 100",
            "legendFormat": "Cache Hit Rate % - {{cache_type}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s"
  }
}
```

### Alerting Rules

```yaml
# monitoring/prometheus/rules/alerts.yml
groups:
  - name: englog.rules
    rules:
      # High-level availability alerts
      - alert: EngLogAPIDown
        expr: up{job="englog-api"} == 0
        for: 30s
        labels:
          severity: critical
          service: api
        annotations:
          summary: "EngLog API is down"
          description: "The EngLog API service has been down for more than 30 seconds."

      - alert: EngLogWorkerDown
        expr: up{job="englog-worker"} == 0
        for: 1m
        labels:
          severity: warning
          service: worker
        annotations:
          summary: "EngLog Worker is down"
          description: "The EngLog Worker service has been down for more than 1 minute."

      # Performance alerts
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(englog_http_request_duration_seconds_bucket[5m])) > 2
        for: 2m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High API response time"
          description: "95th percentile response time is {{ $value }}s for the last 5 minutes."

      - alert: HighErrorRate
        expr: rate(englog_http_requests_total{status_code=~"5.."}[5m]) / rate(englog_http_requests_total[5m]) > 0.05
        for: 1m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "High error rate"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes."

      # AI processing alerts
      - alert: AIProcessingFailures
        expr: rate(englog_ai_processing_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
          service: ai
        annotations:
          summary: "High AI processing failure rate"
          description: "AI processing failure rate is {{ $value }} errors/sec for task type {{ $labels.task_type }}."

      - alert: SlowAIProcessing
        expr: histogram_quantile(0.95, rate(englog_ai_processing_duration_seconds_bucket[5m])) > 60
        for: 5m
        labels:
          severity: warning
          service: ai
        annotations:
          summary: "Slow AI processing"
          description: "95th percentile AI processing time is {{ $value }}s for {{ $labels.task_type }}."

      # Resource alerts
      - alert: HighMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 85
        for: 2m
        labels:
          severity: warning
          service: "{{ $labels.name }}"
        annotations:
          summary: "High memory usage"
          description: "Container {{ $labels.name }} memory usage is {{ $value }}%."

      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.name }}"
        annotations:
          summary: "High CPU usage"
          description: "Container {{ $labels.name }} CPU usage is {{ $value }}%."

      # Database alerts
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 30s
        labels:
          severity: critical
          service: database
        annotations:
          summary: "Database is down"
          description: "PostgreSQL database has been down for more than 30 seconds."

      - alert: SlowDatabaseQueries
        expr: rate(englog_database_query_duration_seconds_sum[5m]) / rate(englog_database_query_duration_seconds_count[5m]) > 1
        for: 3m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "Slow database queries"
          description: "Average database query time is {{ $value }}s for {{ $labels.operation }}."

      # Business metrics alerts
      - alert: LowUserActivity
        expr: englog_users_active_total{time_window="1h"} < 10
        for: 10m
        labels:
          severity: info
          service: business
        annotations:
          summary: "Low user activity"
          description: "Only {{ $value }} active users in the last hour."

      - alert: NoJournalsCreated
        expr: rate(englog_business_journals_created_total[30m]) == 0
        for: 15m
        labels:
          severity: warning
          service: business
        annotations:
          summary: "No journals created"
          description: "No journals have been created in the last 30 minutes."
```

### Alertmanager Configuration

```yaml
# monitoring/alertmanager/alertmanager.yml
global:
  smtp_smarthost: "localhost:587"
  smtp_from: "alerts@englog.com"
  slack_api_url: "{{ .SlackWebhookURL }}"

route:
  group_by: ["alertname", "service"]
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: "default"
  routes:
    - match:
        severity: critical
      receiver: "critical-alerts"
      group_wait: 5s
      repeat_interval: 5m
    - match:
        severity: warning
      receiver: "warning-alerts"
    - match:
        service: business
      receiver: "business-alerts"

receivers:
  - name: "default"
    slack_configs:
      - channel: "#alerts"
        title: "EngLog Alert"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}: {{ .Annotations.description }}{{ end }}"

  - name: "critical-alerts"
    slack_configs:
      - channel: "#critical-alerts"
        title: "🚨 CRITICAL: EngLog Alert"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}: {{ .Annotations.description }}{{ end }}"
        color: "danger"
    pagerduty_configs:
      - routing_key: "{{ .PagerDutyKey }}"
        description: "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"

  - name: "warning-alerts"
    slack_configs:
      - channel: "#alerts"
        title: "⚠️ WARNING: EngLog Alert"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}: {{ .Annotations.description }}{{ end }}"
        color: "warning"
    email_configs:
      - to: "devops@englog.com"
        subject: "EngLog Warning Alert"
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          {{ end }}

  - name: "business-alerts"
    slack_configs:
      - channel: "#product"
        title: "📊 Business Metric Alert"
        text: "{{ range .Alerts }}{{ .Annotations.summary }}: {{ .Annotations.description }}{{ end }}"
        color: "good"

inhibit_rules:
  - source_match:
      severity: "critical"
    target_match:
      severity: "warning"
    equal: ["alertname", "service"]
```

### Distributed Tracing

```go
// internal/tracing/tracer.go
package tracing

import (
    "context"
    "io"

    "github.com/opentracing/opentracing-go"
    "github.com/opentracing/opentracing-go/ext"
    "github.com/uber/jaeger-client-go"
    "github.com/uber/jaeger-client-go/config"
)

func InitTracer(serviceName string) (opentracing.Tracer, io.Closer, error) {
    cfg := config.Configuration{
        ServiceName: serviceName,
        Sampler: &config.SamplerConfig{
            Type:  jaeger.SamplerTypeConst,
            Param: 1, // Sample all requests in development
        },
        Reporter: &config.ReporterConfig{
            LogSpans:           true,
            LocalAgentHostPort: "jaeger:6831",
        },
    }

    tracer, closer, err := cfg.NewTracer()
    if err != nil {
        return nil, nil, err
    }

    opentracing.SetGlobalTracer(tracer)
    return tracer, closer, nil
}

// Middleware for HTTP tracing
func TracingMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        span := opentracing.StartSpan(c.Request.URL.Path)
        defer span.Finish()

        span.SetTag("http.method", c.Request.Method)
        span.SetTag("http.url", c.Request.URL.String())
        span.SetTag("user.id", c.GetString("user_id"))

        ctx := opentracing.ContextWithSpan(c.Request.Context(), span)
        c.Request = c.Request.WithContext(ctx)

        c.Next()

        span.SetTag("http.status_code", c.Writer.Status())
        if c.Writer.Status() >= 400 {
            ext.Error.Set(span, true)
            span.LogKV("error", true, "message", "HTTP error response")
        }
    }
}

// Database tracing helper
func TraceDBQuery(ctx context.Context, operation, table string, fn func() error) error {
    span, ctx := opentracing.StartSpanFromContext(ctx, "db.query")
    defer span.Finish()

    span.SetTag("db.operation", operation)
    span.SetTag("db.table", table)
    span.SetTag("component", "database")

    err := fn()
    if err != nil {
        ext.Error.Set(span, true)
        span.LogKV("error", err.Error())
    }

    return err
}

// AI processing tracing helper
func TraceAIProcessing(ctx context.Context, taskType, provider string, fn func() error) error {
    span, ctx := opentracing.StartSpanFromContext(ctx, "ai.process")
    defer span.Finish()

    span.SetTag("ai.task_type", taskType)
    span.SetTag("ai.provider", provider)
    span.SetTag("component", "ai")

    err := fn()
    if err != nil {
        ext.Error.Set(span, true)
        span.LogKV("error", err.Error())
    }

    return err
}
```

### Structured Logging

```go
// internal/logging/structured.go
package logging

import (
    "context"
    "log/slog"
    "os"

    "github.com/opentracing/opentracing-go"
)

type StructuredLogger struct {
    logger *slog.Logger
}

func NewStructuredLogger(environment string) *StructuredLogger {
    var handler slog.Handler

    if environment == "production" {
        handler = slog.NewJSONHandler(os.Stdout, &slog.HandlerOptions{
            Level: slog.LevelInfo,
        })
    } else {
        handler = slog.NewTextHandler(os.Stdout, &slog.HandlerOptions{
            Level: slog.LevelDebug,
        })
    }

    logger := slog.New(handler)
    return &StructuredLogger{logger: logger}
}

func (l *StructuredLogger) WithContext(ctx context.Context) *slog.Logger {
    span := opentracing.SpanFromContext(ctx)
    if span == nil {
        return l.logger
    }

    if sc, ok := span.Context().(jaeger.SpanContext); ok {
        return l.logger.With(
            "trace_id", sc.TraceID().String(),
            "span_id", sc.SpanID().String(),
        )
    }

    return l.logger
}

func (l *StructuredLogger) LogRequest(ctx context.Context, method, path string, userID string, duration time.Duration, status int) {
    logger := l.WithContext(ctx)

    logger.Info("http_request",
        "method", method,
        "path", path,
        "user_id", userID,
        "duration_ms", duration.Milliseconds(),
        "status", status,
        "timestamp", time.Now(),
    )
}

func (l *StructuredLogger) LogAIProcessing(ctx context.Context, taskType, provider string, duration time.Duration, err error) {
    logger := l.WithContext(ctx)

    if err != nil {
        logger.Error("ai_processing_error",
            "task_type", taskType,
            "provider", provider,
            "duration_ms", duration.Milliseconds(),
            "error", err.Error(),
        )
    } else {
        logger.Info("ai_processing_success",
            "task_type", taskType,
            "provider", provider,
            "duration_ms", duration.Milliseconds(),
        )
    }
}

func (l *StructuredLogger) LogBusinessEvent(ctx context.Context, event string, userID string, metadata map[string]any) {
    logger := l.WithContext(ctx)

    attrs := []slog.Attr{
        slog.String("event", event),
        slog.String("user_id", userID),
    }

    for k, v := range metadata {
        attrs = append(attrs, slog.Any(k, v))
    }

    logger.LogAttrs(ctx, slog.LevelInfo, "business_event", attrs...)
}
```

### Docker Compose Monitoring Stack

```yaml
# docker-compose.monitoring.yml
version: "3.8"

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    networks:
      - monitoring

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    ports:
      - "16686:16686"
      - "6831:6831/udp"
    environment:
      - COLLECTOR_ZIPKIN_HTTP_PORT=9411
    networks:
      - monitoring

  # Exporters
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    networks:
      - monitoring

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: postgres-exporter
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://englog:password@postgres:5432/englog?sslmode=disable
    networks:
      - monitoring
      - englog-network

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
    networks:
      - monitoring
      - englog-network

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:

networks:
  monitoring:
    driver: bridge
  englog-network:
    external: true
```

### Health Check Scripts

```bash
#!/bin/bash
# scripts/comprehensive-health-check.sh
set -euo pipefail

ENVIRONMENT="${1:-production}"
ALERT_WEBHOOK="${2:-}"

echo "🏥 Running comprehensive health checks for $ENVIRONMENT..."

# Define endpoints based on environment
if [[ "$ENVIRONMENT" == "production" ]]; then
    API_URL="https://api.englog.com"
    WEB_URL="https://englog.com"
    PROMETHEUS_URL="http://prometheus:9090"
else
    API_URL="https://api-staging.englog.com"
    WEB_URL="https://staging.englog.com"
    PROMETHEUS_URL="http://prometheus:9090"
fi

HEALTH_STATUS=0
RESULTS=()

# Function to check endpoint health
check_endpoint() {
    local name="$1"
    local url="$2"
    local expected_status="$3"

    echo "Checking $name..."
    local status_code
    status_code=$(curl -s -o /dev/null -w "%{http_code}" "$url" || echo "000")

    if [[ "$status_code" == "$expected_status" ]]; then
        echo "✅ $name: OK ($status_code)"
        RESULTS+=("✅ $name: OK")
    else
        echo "❌ $name: FAILED ($status_code)"
        RESULTS+=("❌ $name: FAILED ($status_code)")
        HEALTH_STATUS=1
    fi
}

# Function to check metric value
check_metric() {
    local name="$1"
    local query="$2"
    local threshold="$3"
    local operator="$4"

    echo "Checking $name..."
    local value
    value=$(curl -s "$PROMETHEUS_URL/api/v1/query?query=$query" | jq -r '.data.result[0].value[1] // "0"')

    if [[ "$operator" == "lt" ]] && (( $(echo "$value < $threshold" | bc -l) )); then
        echo "✅ $name: OK ($value < $threshold)"
        RESULTS+=("✅ $name: OK ($value)")
    elif [[ "$operator" == "gt" ]] && (( $(echo "$value > $threshold" | bc -l) )); then
        echo "✅ $name: OK ($value > $threshold)"
        RESULTS+=("✅ $name: OK ($value)")
    else
        echo "❌ $name: FAILED ($value)"
        RESULTS+=("❌ $name: FAILED ($value)")
        HEALTH_STATUS=1
    fi
}

# Basic endpoint checks
check_endpoint "API Health" "$API_URL/health" "200"
check_endpoint "Web Frontend" "$WEB_URL" "200"
check_endpoint "API Database Health" "$API_URL/health/db" "200"
check_endpoint "API Redis Health" "$API_URL/health/redis" "200"

# Performance checks
check_metric "API Response Time" 'histogram_quantile(0.95, rate(englog_http_request_duration_seconds_bucket[5m]))' "2.0" "lt"
check_metric "Error Rate" 'rate(englog_http_requests_total{status_code=~"5.."}[5m]) / rate(englog_http_requests_total[5m]) * 100' "5.0" "lt"

# Resource checks
check_metric "CPU Usage" 'rate(container_cpu_usage_seconds_total{name=~".*api.*"}[5m]) * 100' "80.0" "lt"
check_metric "Memory Usage" '(container_memory_usage_bytes{name=~".*api.*"} / container_spec_memory_limit_bytes{name=~".*api.*"}) * 100' "85.0" "lt"

# Business metrics checks
check_metric "Active Users (1h)" 'englog_users_active_total{time_window="1h"}' "1" "gt"

# Generate report
echo ""
echo "📊 Health Check Summary:"
echo "========================"
for result in "${RESULTS[@]}"; do
    echo "$result"
done

if [[ $HEALTH_STATUS -eq 0 ]]; then
    echo ""
    echo "🎉 All health checks passed!"
    STATUS_EMOJI="✅"
    STATUS_TEXT="HEALTHY"
else
    echo ""
    echo "⚠️ Some health checks failed!"
    STATUS_EMOJI="❌"
    STATUS_TEXT="UNHEALTHY"
fi

# Send alert if webhook provided
if [[ -n "$ALERT_WEBHOOK" ]]; then
    curl -X POST "$ALERT_WEBHOOK" \
        -H 'Content-type: application/json' \
        -d "{
            \"text\": \"$STATUS_EMOJI Health Check $STATUS_TEXT ($ENVIRONMENT)\",
            \"attachments\": [{
                \"color\": \"$([ $HEALTH_STATUS -eq 0 ] && echo 'good' || echo 'danger')\",
                \"fields\": [{
                    \"title\": \"Environment\",
                    \"value\": \"$ENVIRONMENT\",
                    \"short\": true
                }, {
                    \"title\": \"Status\",
                    \"value\": \"$STATUS_TEXT\",
                    \"short\": true
                }, {
                    \"title\": \"Details\",
                    \"value\": \"$(printf '%s\\n' \"${RESULTS[@]}\")\",
                    \"short\": false
                }]
            }]
        }"
fi

exit $HEALTH_STATUS
```

## Acceptance Criteria

### Core Requirements

- [ ] Prometheus metrics collection from all services
- [ ] Grafana dashboards for system and business metrics
- [ ] Alertmanager configuration with multi-channel notifications
- [ ] Distributed tracing with Jaeger integration
- [ ] Structured logging with correlation IDs
- [ ] Health check automation and reporting
- [ ] Performance monitoring with SLA tracking
- [ ] Resource utilization monitoring and alerting

### Metrics Requirements

- [ ] HTTP request metrics (rate, duration, errors)
- [ ] Business metrics (users, journals, AI processing)
- [ ] Infrastructure metrics (CPU, memory, disk, network)
- [ ] Database performance metrics
- [ ] Cache performance metrics
- [ ] AI processing metrics with provider breakdown
- [ ] Custom application metrics
- [ ] Security and compliance metrics

### Alerting Requirements

- [ ] Critical alerts with immediate escalation
- [ ] Warning alerts with appropriate thresholds
- [ ] Business metric alerts for anomaly detection
- [ ] Alert routing based on severity and service
- [ ] Alert acknowledgment and resolution tracking
- [ ] Alert fatigue prevention with intelligent grouping
- [ ] Integration with PagerDuty for critical issues
- [ ] Slack notifications for team awareness

### Observability Requirements

- [ ] Request tracing across all services
- [ ] Performance bottleneck identification
- [ ] Error tracking with context
- [ ] User journey monitoring
- [ ] Dependency mapping and health
- [ ] Log correlation with metrics and traces
- [ ] Root cause analysis capabilities
- [ ] Trend analysis and capacity planning

## Task Dependencies

- **Prerequisite:** MVP-022 (Docker Compose Production Setup), MVP-023 (CI/CD Pipeline Implementation)
- **Enables:** Complete production observability and monitoring
- **Integrates:** All other MVP tasks for comprehensive monitoring

## Definition of Done

- [ ] Monitoring stack deployed and functional
- [ ] All services reporting metrics to Prometheus
- [ ] Grafana dashboards created and accessible
- [ ] Alerting rules configured and tested
- [ ] Distributed tracing operational across services
- [ ] Structured logging implemented with correlation
- [ ] Health checks automated and reporting
- [ ] Documentation complete for monitoring operations
- [ ] Team training completed on monitoring tools
- [ ] Production monitoring validated and operational

## Notes

- Configure retention policies for metrics and logs
- Implement metric aggregation for long-term storage
- Consider using Prometheus federation for scale
- Set up log rotation and archival policies
- Implement security for monitoring endpoints
- Plan for monitoring infrastructure scaling
- Document incident response procedures
- Establish monitoring review and optimization schedule
